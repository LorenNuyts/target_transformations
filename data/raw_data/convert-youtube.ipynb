{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "File copied from Laurens' bitboost project"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T12:55:02.799714Z",
     "start_time": "2024-05-03T12:55:02.786679Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube\n",
    "\n",
    "## Load and transform the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T12:55:03.314095Z",
     "start_time": "2024-05-03T12:55:02.800836Z"
    }
   },
   "source": [
    "# Source:\n",
    "# =======\n",
    "# https://www.kaggle.com/datasnaek/youtube-new\n",
    "\n",
    "import io, os, sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as fetext\n",
    "# import qgrid\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# base_path = \"home/loren/Code/Evaluation/data/raw_data\"\n",
    "base_path = \".\"\n",
    "workdir = os.path.join(base_path, \"data\", \"youtube\")\n",
    "source_data_h5 = \"youtube.h5\"\n",
    "source_data_h5_path = os.path.join(base_path, \"..\", \"datasets\", source_data_h5)\n",
    "h5_key = \"dataset\"\n",
    "\n",
    "# bitboost_path = \"..\"\n",
    "\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T13:01:36.943725Z",
     "start_time": "2024-05-03T13:01:02.858374Z"
    }
   },
   "source": [
    "# Prep the source data file if prepped files do not exist\n",
    "if not os.path.isfile(source_data_h5_path) or True:\n",
    "    df1 = pd.read_csv(os.path.join(workdir, \"CAvideos.csv\"))\n",
    "    df2 = pd.read_csv(os.path.join(workdir, \"GBvideos.csv\"))\n",
    "    df3 = pd.read_csv(os.path.join(workdir, \"USvideos.csv\"))\n",
    "    df1[\"country\"] = 0\n",
    "    df2[\"country\"] = 1\n",
    "    df3[\"country\"] = 2\n",
    "    df = pd.concat([df1, df2, df3], axis=0, ignore_index=True)\n",
    "    \n",
    "    display(df1.shape)\n",
    "    display(df2.shape)\n",
    "    display(df3.shape)\n",
    "    display(df.shape)\n",
    "    \n",
    "    df.drop(columns=['video_id', 'thumbnail_link'], inplace=True)\n",
    "    display(df.columns)\n",
    "\n",
    "    print(\"[ ] cleaning tags and description\")\n",
    "    re_tag1 = re.compile('(\\||/)')\n",
    "    re_tag2 = re.compile('(\"|\\'|\\[none\\])')\n",
    "    df['tags'] = df['tags'].apply(lambda s: re_tag2.sub('', re_tag1.sub(' ', s)))\n",
    "    df['description'] = df['description'].apply(lambda s: s if isinstance(s, str) else '' )\n",
    "    \n",
    "    print(\"[ ] generating word corpus\")\n",
    "    corpus1 = list(df['title'])\n",
    "    corpus2 = list(df['channel_title'])\n",
    "    corpus3 = list(df['tags'])\n",
    "\n",
    "    corpus = list(map(lambda t: t[0] + ' ' + t[1] + ' ' + t[2],\n",
    "                      zip(corpus1, corpus2, corpus3)))\n",
    "    \n",
    "    print(\"[ ] generating term-document matrix\")\n",
    "    vectorizer = fetext.CountVectorizer(strip_accents='unicode', min_df=0.01, max_df=0.9, ngram_range=(1,4),\n",
    "                                        binary=True, lowercase=True)\n",
    "    tdm = vectorizer.fit_transform(corpus)\n",
    "    tdm_dense = tdm.todense().astype(np.uint8)\n",
    "    \n",
    "    print(\"[ ] generating n° word features\")\n",
    "    features = {}\n",
    "    re_space = re.compile('\\s+')\n",
    "\n",
    "    features['title_nchars'] = list(map(len, df['title']))\n",
    "    features['title_nwords'] = list(map(lambda x: len(re_space.split(x)), df['title']))\n",
    "    features['channel_nchars'] = list(map(len, df['channel_title']))\n",
    "    features['channel_nwords'] = list(map(lambda x: len(re_space.split(x)), df['channel_title']))\n",
    "    features['descr_nchars'] = list(map(len, df['description']))\n",
    "    features['descr_nwords'] = list(map(lambda x: len(re_space.split(x)), df['description']))\n",
    "    features['ntags'] = list(map(lambda x: len(re_space.split(x)), df['tags']))\n",
    "    \n",
    "    print(\"[ ] generating trend date features\")\n",
    "    features['trend_year'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').year, df['trending_date']))\n",
    "    features['trend_month'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').month, df['trending_date']))\n",
    "    features['trend_day'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').day, df['trending_date']))\n",
    "    features['trend_wday'] = list(map(lambda x: datetime.strptime(x, '%y.%d.%m').weekday(), df['trending_date']))\n",
    "    \n",
    "    print(\"[ ] generating publish date features\")\n",
    "    features['publish_year'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').year, df['publish_time']))\n",
    "    features['publish_month'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').month, df['publish_time']))\n",
    "    features['publish_day'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').day, df['publish_time']))\n",
    "    features['publish_hour'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').hour, df['publish_time']))\n",
    "    features['publish_wday'] = list(map(lambda x: datetime.strptime(x[0:13], '%Y-%m-%dT%H').weekday(), df['publish_time']))\n",
    "        \n",
    "    print(\"[ ] generating transformed features\")\n",
    "    # features['dislikeslg'] = np.log(df['dislikes'] + 1, dtype=np.float32)\n",
    "    # features['likeslg'] = np.log(df['likes'] + 1, dtype=np.float32)\n",
    "    # features['dislikeratiolg'] = np.log((df['dislikes'] + 1) / (df['likes'] + df['dislikes'] + 1), dtype=np.float32)\n",
    "    # features['cmtslg'] = np.log(df['comment_count'] + 1, dtype=np.float32)\n",
    "    # features['likepcmtlg'] = np.log((df['comment_count'] + 1) / (df['likes'] + df['dislikes'] + 1), dtype=np.float32)\n",
    "    # #features[\"viewslg\"] = np.log(df['views'] + 1, dtype=np.float32)\n",
    "    \n",
    "    features['dislikes'] = df['dislikes']\n",
    "    features['likes'] = df['likes']\n",
    "    # features['dislikeratio'] = df['dislikes'] / (df['likes'] + df['dislikes'])\n",
    "    features['cmts'] = df['comment_count']\n",
    "    # features['likepcmt'] = df['comment_count']  / (df['likes'] + df['dislikes'] )\n",
    "   \n",
    "    print(\"[ ] generating categorical features\")\n",
    "    cat_features = {}\n",
    "    cat_features['cat_id'] = list(df['category_id'])\n",
    "    cat_features['country'] = list(df['country'])\n",
    "    cat_features['cmtsdis'] = list(map(lambda x: 1 if x else 0, df['comments_disabled']))\n",
    "    cat_features['likedis'] = list(map(lambda x: 1 if x else 0, df['ratings_disabled']))\n",
    "    cat_features['err'] = list(map(lambda x: 1 if x else 0, df['video_error_or_removed']))\n",
    "\n",
    "    print(\"[ ] generating target\")\n",
    "    #features['target'] = (df[\"likes\"] > df[\"dislikes\"]).astype(np.uint8)\n",
    "    #features['target'] = np.log(((df[\"likes\"]+1) / (df[\"dislikes\"]+1)), dtype=np.float32)\n",
    "    # features[\"viewslg\"] = np.log10(df['views'] + 1, dtype=np.float32)\n",
    "    features[\"views\"] = df['views']\n",
    "    \n",
    "    print(\"[ ] combining dataframe\")\n",
    "    n = df.shape[0]\n",
    "    colnames = list(map(lambda x: \"txt_\"+re_space.sub('_', x), vectorizer.get_feature_names_out()))\n",
    "    df1 = pd.DataFrame(tdm_dense, columns=colnames, index=range(n), dtype=np.uint32)\n",
    "    df2 = pd.DataFrame(features, index=range(n), dtype=np.float32)\n",
    "    df3 = pd.DataFrame(cat_features, index=range(n), dtype=np.uint32)\n",
    "    df_comb  = pd.concat([df1, df3, df2], axis=1)\n",
    "    display(df1.shape)\n",
    "    display(df2.shape)\n",
    "    display(df3.shape)\n",
    "    display(df_comb.shape)\n",
    "\n",
    "    print(\"[ ] write to hdf5\")\n",
    "    df_comb.to_hdf(source_data_h5_path, h5_key, complevel=9)\n",
    "    \n",
    "    print(\"[ ] done\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40881, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(38916, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(40949, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(120746, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['trending_date', 'title', 'channel_title', 'category_id',\n",
       "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
       "       'comments_disabled', 'ratings_disabled', 'video_error_or_removed',\n",
       "       'description', 'country'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ] cleaning tags and description\n",
      "[ ] generating word corpus\n",
      "[ ] generating term-document matrix\n",
      "[ ] generating n° word features\n",
      "[ ] generating trend date features\n",
      "[ ] generating publish date features\n",
      "[ ] generating transformed features\n",
      "[ ] generating categorical features\n",
      "[ ] generating target\n",
      "[ ] combining dataframe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120746, 373)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(120746, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(120746, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(120746, 398)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ] write to hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87702/3008426657.py:105: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
      "  df_comb.to_hdf(source_data_h5_path, h5_key, complevel=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ] done\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
